# YSDA Natural Language Processing course

* Welcome to my personal repository, where I've compiled my solutions and comprehensive notes for the YSDA Natural Language Processing (NLP) course. This resource is a product of my self-directed learning journey through the intricacies of NLP.
* Explore my notes [here](https://github.com/Grafit24/Summary-Machine-Learning/tree/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP).
* Find my solutions organized by week within the `week_*` folders.

# Syllabus

* ✅ [__week01__](./week01_embeddings) __Word Embeddings__
  * ✅ Lecture: Word embeddings. Distributional semantics. Count-based (pre-neural) methods. Word2Vec: learn vectors. GloVe: count, then learn. Evaluation: intrinsic vs extrinsic. Analysis and Interpretability. [Interactive lecture materials and more.](https://lena-voita.github.io/nlp_course.html#preview_word_emb).
  * ✅ Seminar: Playing with word and sentence embeddings
  * ✅ Homework: Embedding-based machine translation system
  * ✅ My summary of lecture in [pdf](./week01_embeddings/word_embeddings_summary.pdf) and [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/blob/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP/Word%20Embeddings.md).

* ✅ [__week02__](./week02_classification) __Text Classification__
  * ✅ Lecture: Text classification: introduction and datasets. General framework: feature extractor + classifier. Classical approaches: Naive Bayes, MaxEnt (Logistic Regression), SVM. Neural Networks: General View, Convolutional Models, Recurrent Models. Practical Tips: Data Augmentation. Analysis and Interpretability. [Interactive lecture materials and more.](https://lena-voita.github.io/nlp_course.html#preview_text_clf)
  * ✅ Seminar: Text classification with convolutional NNs.
  * ✅ Homework: Statistical & neural text classification.
  * ✅ My summary of lectures in [obsidian canvas](https://github.com/Grafit24/Summary-Machine-Learning/blob/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP/Text%20Classification.canvas)
  
* ✅ [__week03__](./week03_lm) __Language Modeling__
  * ✅ Lecture: Language Modeling: what does it mean? Left-to-right framework. N-gram language models. Neural Language Models: General View, Recurrent Models, Convolutional Models. Evaluation. Practical Tips: Weight Tying. Analysis and Interpretability. [Interactive lecture materials and more.](https://lena-voita.github.io/nlp_course.html#preview_lang_models)
  * ✅ Seminar: Build a N-gram language model from scratch
  * ✅ Homework: Neural LMs & smoothing in count-based models.
  * ✅ My summary of lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/blob/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP/Language%20Modeling.md)
  
* ✅ [__week04__](./week04_seq2seq) __Seq2seq and Attention__
  * ✅ Lecture: Seq2seq Basics: Encoder-Decoder framework, Training, Simple Models, Inference (e.g., beam search). Attention: general, score functions, models. Transformer: self-attention, masked self-attention, multi-head attention; model architecture. Subword Segmentation (BPE). Analysis and Interpretability: functions of attention heads; probing for linguistic structure. [Interactive lecture materials and more.](https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn)
  * ✅ Seminar: Basic sequence to sequence model
  * ✅ Homework: Machine translation with attention
  * ✅ My summary of lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/blob/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP/Language%20Modeling.md)
  
* ✅ [__week05__](./week05_transfer) __Transfer Learning__
  * ✅ Lecture: What is Transfer Learning? Great idea 1: From Words to Words-in-Context (CoVe, ELMo). Great idea 2: From Replacing Embeddings to Replacing Models (GPT, BERT). (A Bit of) Adaptors. Analysis and Interpretability. [Interactive lecture materials and more.](https://lena-voita.github.io/nlp_course.html#preview_transfer)
  * ✅ My summary of all lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/tree/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP)

* ✅ [__week06__](./week06_da) __Domain Adaptation__
  * ✅ Lecture: General theory. Instance weighting. Proxy-labels methods. Feature matching methods. Distillation-like methods.
  * ✅ Seminar+Homework: BERT-based NER domain adaptation
  * ✅ My summary of all lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/tree/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP)
  
* ✅ [__week07__](./week07_compression) __Model deployment, compression & acceleration__
  * ✅ Lecture: how nlp models get deployed; how (and why) make your model faster and/or smaller
  * No assignment this time; instead, we showcase running a simple ML model in your browser
  * ✅ My summary of all lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/tree/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP)

* ✅ [__week08__](./week08_llm) __Large Language Models & Their Implications__
  * ✅ Lecture: more BERTology; Large language models: GPT-3, OPT, BLOOM; in-context learning, prompt engineering, parameter-efficient fine-tuning
  * Practice: prompt engineering and LoRA on a 6.7B model in colab
  * ✅ My summary of all lectures [obsidian md](https://github.com/Grafit24/Summary-Machine-Learning/tree/bfaff955cd509a1f776e5ace41428b043d7d02fa/NLP)

More TBA

# Contributors & course staff

Course materials and teaching performed by

* [Elena Voita](https://lena-voita.github.io) - course admin, lectures, seminars, homeworks
* [Boris Kovarsky](https://github.com/kovarsky) - lectures, seminars, homeworks
* [David Talbot](https://github.com/drt7) - lectures, seminars, homeworks
* [Sergey Gubanov](https://github.com/esgv) - lectures, seminars, homeworks
* [Just Heuristic](https://github.com/justheuristic) - lectures, seminars, homeworks
