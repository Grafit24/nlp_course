{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE00uQW6-wXP"
      },
      "source": [
        "### N-gram language models or how to write scientific papers (4 pts)\n",
        "\n",
        "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
        "\n",
        "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
        "\n",
        "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
        "\n",
        "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GoUzqRXw-wXV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "sCDdiWuy-wXX",
        "outputId": "59429fed-e341-4ab9-c059-2575596af09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-01 15:49:04--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2023-12-01 15:49:05--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com/cd/0/get/CInuYLUAXy8YD0MGNwvY0VIqTXObBz6C1YYrAVXeQVQGzl7FoA_rQS2k40cE-56QTRiabr8YQFewud7qKUkY0p5pmMx4FhLJUS5mwOqMbp0PfYjVsQp74x9lk0EB5C8GeEw/file?dl=1# [following]\n",
            "--2023-12-01 15:49:05--  https://uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com/cd/0/get/CInuYLUAXy8YD0MGNwvY0VIqTXObBz6C1YYrAVXeQVQGzl7FoA_rQS2k40cE-56QTRiabr8YQFewud7qKUkY0p5pmMx4FhLJUS5mwOqMbp0PfYjVsQp74x9lk0EB5C8GeEw/file?dl=1\n",
            "Resolving uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com (uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com (uc6f8e4f1a32ea7c0216c9377b0f.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  7.00MB/s    in 2.6s    \n",
            "\n",
            "2023-12-01 15:49:09 (7.00 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "2992   [{'name': 'Haanvid Lee'}, {'name': 'Minju Jung...    5  1602.01921v3   \n",
              "11961  [{'name': 'Renbo Zhao'}, {'name': 'William B. ...   19  1705.06884v2   \n",
              "20765  [{'name': 'Alex Kuefler'}, {'name': 'Jeremy Mo...   24  1701.06699v1   \n",
              "15608  [{'name': 'Peilu Wang'}, {'name': 'Yao Qian'},...    1  1511.00215v1   \n",
              "6776   [{'name': 'Kevin Bello'}, {'name': 'Jean Honor...    2  1706.00754v3   \n",
              "\n",
              "                                                    link  month  \\\n",
              "2992   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "11961  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
              "20765  [{'rel': 'alternate', 'href': 'http://arxiv.or...      1   \n",
              "15608  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "6776   [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "\n",
              "                                                 summary  \\\n",
              "2992   The current paper proposes a novel neural netw...   \n",
              "11961  We propose a unified framework to speed up the...   \n",
              "20765  The ability to accurately predict and simulate...   \n",
              "15608  Bidirectional Long Short-Term Memory Recurrent...   \n",
              "6776   Causal discovery from empirical data is a fund...   \n",
              "\n",
              "                                                     tag  \\\n",
              "2992   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "11961  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "20765  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "15608  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "6776   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "2992   Recognition of Visually Perceived Compositiona...  2016  \n",
              "11961  A Unified Framework for Stochastic Matrix Fact...  2017  \n",
              "20765  Imitating Driver Behavior with Generative Adve...  2017  \n",
              "15608  A Unified Tagging Solution: Bidirectional LSTM...  2015  \n",
              "6776   Learning causal Bayes networks using intervent...  2017  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7be025b-6bab-4e7a-9f51-ce7eb2c89089\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2992</th>\n",
              "      <td>[{'name': 'Haanvid Lee'}, {'name': 'Minju Jung...</td>\n",
              "      <td>5</td>\n",
              "      <td>1602.01921v3</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>The current paper proposes a novel neural netw...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Recognition of Visually Perceived Compositiona...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11961</th>\n",
              "      <td>[{'name': 'Renbo Zhao'}, {'name': 'William B. ...</td>\n",
              "      <td>19</td>\n",
              "      <td>1705.06884v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>We propose a unified framework to speed up the...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>A Unified Framework for Stochastic Matrix Fact...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20765</th>\n",
              "      <td>[{'name': 'Alex Kuefler'}, {'name': 'Jeremy Mo...</td>\n",
              "      <td>24</td>\n",
              "      <td>1701.06699v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>1</td>\n",
              "      <td>The ability to accurately predict and simulate...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Imitating Driver Behavior with Generative Adve...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15608</th>\n",
              "      <td>[{'name': 'Peilu Wang'}, {'name': 'Yao Qian'},...</td>\n",
              "      <td>1</td>\n",
              "      <td>1511.00215v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>Bidirectional Long Short-Term Memory Recurrent...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Unified Tagging Solution: Bidirectional LSTM...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6776</th>\n",
              "      <td>[{'name': 'Kevin Bello'}, {'name': 'Jean Honor...</td>\n",
              "      <td>2</td>\n",
              "      <td>1706.00754v3</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>Causal discovery from empirical data is a fund...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Learning causal Bayes networks using intervent...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7be025b-6bab-4e7a-9f51-ce7eb2c89089')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7be025b-6bab-4e7a-9f51-ce7eb2c89089 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7be025b-6bab-4e7a-9f51-ce7eb2c89089');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5effbed-c8e3-4caf-a832-bb2830376604\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5effbed-c8e3-4caf-a832-bb2830376604')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5effbed-c8e3-4caf-a832-bb2830376604 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35N06Glo-wXY",
        "outputId": "4a1b5a20-76cf-48d5-b35d-ff00d08f2d32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# assemble lines: concatenate title and description\n",
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace('\\n', ' '), axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C24KuL4F-wXY"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zBUde8OW-wXZ"
      },
      "outputs": [],
      "source": [
        "# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "lines = [\" \".join(tokenizer.tokenize(line.lower())) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tf3v_YOH-wXa"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4RjUxLh-wXb"
      },
      "source": [
        "### N-Gram Language Model (1point)\n",
        "\n",
        "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
        "\n",
        "It can do so by following the chain rule:\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$\n",
        "\n",
        "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
        "\n",
        "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
        "\n",
        "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words.\n",
        "\n",
        "$$\n",
        "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
        "$$\n",
        "\n",
        "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52GLu_1i-wXc"
      },
      "source": [
        "The first stage to building such a model is counting all word occurences given N-1 previous words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V12cyABt-wXc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# special tokens:\n",
        "# - unk represents absent tokens,\n",
        "# - eos is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "\n",
        "    When building counts, please consider the following two edge cases\n",
        "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
        "      empty prefix: \"\" -> (UNK, UNK)\n",
        "      short prefix: \"the\" -> (UNK, the)\n",
        "      long prefix: \"the new approach\" -> (new, approach)\n",
        "    - you should add a special token, EOS, at the end of each sequence\n",
        "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
        "      count the probability of this token just like all others.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split(\" \")\n",
        "        tokens.append(EOS)\n",
        "        for idx in range(len(tokens)):\n",
        "            prev_tokens = tuple([(UNK if idx - j < 0 else tokens[idx - j]) for j in range(n - 1, 0, -1)])\n",
        "            counts[prev_tokens].update([tokens[idx]])\n",
        "\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AXRcUaiJ-wXc"
      },
      "outputs": [],
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmM8ekIN-wXd"
      },
      "source": [
        "Once we can count N-grams, we can build a probabilistic language model.\n",
        "The simplest way to compute probabilities is in proporiton to counts:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gzQbzzx9-wXe"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\"\n",
        "        Train a simple count-based language model:\n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "\n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        # compute token proabilities given counts\n",
        "        self.probs = defaultdict(Counter)\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "\n",
        "        # populate self.probs with actual probabilities\n",
        "        for ngram, counter in counts.items():\n",
        "            total = counter.total()\n",
        "            self.probs[ngram] = {key: counter[key] / total  for key in counter.keys()}\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_uM5FLz-wXf"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y3dNLq7i-wXf"
      },
      "outputs": [],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkfXufTy-wXg"
      },
      "source": [
        "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "geGKctBY-wXg"
      },
      "outputs": [],
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLj1xjKf-wXg"
      },
      "source": [
        "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "__forever:__\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "\n",
        "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sbu1lF8u-wXg"
      },
      "outputs": [],
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    probs = lm.get_possible_next_tokens(prefix)\n",
        "    tokens = list(probs.keys())\n",
        "    probabilities = list(probs.values())\n",
        "    assert temperature >= 0\n",
        "    if temperature > 0:\n",
        "        weights = np.power(np.array(list(probs.values())), (1 / temperature))\n",
        "        probabilities = weights / weights.sum()\n",
        "        next_token = np.random.choice(tokens, p=probabilities)\n",
        "    else:\n",
        "        next_token = tokens[np.argmax(probabilities)]\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y26MhRBU-wXg",
        "outputId": "1c66ffd6-dae9-4996-afca-32659d72125d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4nm-Q_-wXh"
      },
      "source": [
        "Let's have fun with this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "3A6yX0jk-wXh",
        "outputId": "660421d3-be9a-4372-a93c-4a36468bcac1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "equation parsing and production of the general theory for its economic consequences . knowledge can improve the classification task where it is often done best by taking the partitioned global address space ( aka the turing test in the low - rank decompositions , which is inferior to caffe . _EOS_"
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "prefix = 'equation' # <- your ideas :)\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "Markdown(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "QhruTuG9-wXh",
        "outputId": "5b9d8d95-bdd6-42ea-fd55-0f68a5bd209c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "bridging the gap between the two most important steps in the context of multi - scale datasets and demonstrate the effectiveness of our approach is based on the other hand , we propose a novel method for the proposed approach is to derive a novel approach to learning from the scene . this is achieved by using color : color parameterizes the unknown signal alleviates the risk of overfitting in neural networks ( cnn ) to obtain the final classification . the proposed approach is to generate the final results . _EOS_"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "prefix = 'bridging the' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "Markdown(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOwN2bnT-wXh"
      },
      "source": [
        "__More in the homework:__ nucleous sampling, top-k sampling, beam search(not for the faint of heart)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnC1sV_c-wXh"
      },
      "source": [
        "### Evaluating language models: perplexity (1point)\n",
        "\n",
        "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
        "\n",
        "To compute perplexity on one sentence, use:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "\n",
        "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
        "\n",
        "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Qmk-VWZ0-wXh"
      },
      "outputs": [],
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: a list of strings with space-separated tokens\n",
        "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
        "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
        "\n",
        "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
        "\n",
        "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
        "    \"\"\"\n",
        "    logs_sum = 0.0\n",
        "    N = 0\n",
        "    print(\"Calculate perplexity...\")\n",
        "    for line in tqdm(lines):\n",
        "        tokens = line.split(\" \")\n",
        "        for idx in range(len(tokens) + 1):\n",
        "            gap = idx - (lm.n - 1)\n",
        "            if gap >= 0:\n",
        "                prefix = \" \".join(tokens[gap:idx])\n",
        "            else:\n",
        "                # gap is negative\n",
        "                unk_prefix = \" \".join([UNK for _ in range(-gap)])\n",
        "                prefix = \" \".join([unk_prefix, *tokens[:idx]])\n",
        "            next_token = tokens[idx] if len(tokens) > idx else EOS\n",
        "            prob = lm.get_next_token_prob(prefix, next_token)\n",
        "            logs_sum += max(np.log(prob), min_logprob) if prob != 0 else min_logprob\n",
        "            N += 1\n",
        "\n",
        "    return np.exp((-1 / N) * logs_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc344aXM-wXi",
        "outputId": "d0f28b98-8bac-4227-e7bd-bb05f2e00f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 6835.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 7302.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 5214.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4750.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX5rPfa-wXi"
      },
      "source": [
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMorhH9X-wXi",
        "outputId": "031b2375-d725-463e-b649-0ae4a879356a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:08<00:00, 1169.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 1832.23136\n",
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:07<00:00, 1442.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 2, Perplexity = 85653987.28774\n",
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:10<00:00, 959.38it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 3, Perplexity = 61999196259042902147072.00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hRvziNVO-wXi"
      },
      "outputs": [],
      "source": [
        "# whoops, it just blew up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSXMWaUh-wXi"
      },
      "source": [
        "### LM Smoothing\n",
        "\n",
        "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
        "\n",
        "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
        "\n",
        "Here's an example code we've implemented for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WPJbjAxC-wXi"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvbCcQN-wXj"
      },
      "source": [
        "**Disclaimer**: the implementation above assumes all words unknown within a given context to be equally likely, *as well as the words outside of vocabulary*. Therefore, its' perplexity will be lower than it should when encountering such words. Therefore, comparing it with a model with less unknown words will not be fair. When implementing your own smoothing, you may handle this by adding a virtual `UNK` token of non-zero probability. Technically, this will result in a model where probabilities do not add up to $1$, but it is close enough for a practice excercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7euXeocR-wXj"
      },
      "outputs": [],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTrcMsUo-wXj",
        "outputId": "950fd6bd-610a-4365-e27f-0d2f6272edac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:18<00:00, 539.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 977.67559\n",
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:18<00:00, 549.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 2, Perplexity = 470.48021\n",
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:17<00:00, 571.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 3, Perplexity = 3679.44765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "xOE8LH9c-wXj",
        "outputId": "613dcc62-4a28-4083-931f-34ad5240e23f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "bridging the labeling cbts homo websites eficientes trees unlearn penetration fars drawn combinabilities indicator qq atnosferes identikit _EOS_"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# optional: try to sample tokens from such a model\n",
        "prefix = 'bridging the'\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "Markdown(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf-J4rr4-wXj"
      },
      "source": [
        "### Kneser-Ney smoothing (2 points)\n",
        "\n",
        "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
        "\n",
        "\n",
        "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
        "\n",
        "It can be computed recurrently, for n>1:\n",
        "\n",
        "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
        "\n",
        "where\n",
        "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
        "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
        "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
        "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
        "\n",
        "See lecture slides or wiki for more detailed formulae.\n",
        "\n",
        "__Your task__ is to\n",
        "- implement KneserNeyLanguageModel\n",
        "- test it on 1-3 gram language models\n",
        "- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class KneserNeyLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
        "    def __init__(self, lines, n, delta=0.9):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.counts = counts\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "        self.len_counts = {}\n",
        "        self.total_counts = {}\n",
        "        if self.n - 1 > 0:\n",
        "            self.models_m = KneserNeyLanguageModel(lines, self.n - 1, delta=delta)\n",
        "\n",
        "            print(f\"Train {self.n}-gram model\")\n",
        "            for prefix in tqdm(counts):\n",
        "                token_counts = counts[prefix]\n",
        "                total_count = sum(token_counts.values())\n",
        "                result = {token: (max(token_counts.get(token, 0) - delta, 0)) / total_count for token in token_counts}\n",
        "                self.probs[prefix] = result\n",
        "                self.len_counts[prefix] = len(token_counts)\n",
        "                self.total_counts[prefix] = total_count\n",
        "\n",
        "        else:\n",
        "            self.probs[tuple()] = self.calculate_p0(lines, self.vocab)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_p0(lines, vocab):\n",
        "        counts = Counter()\n",
        "        probs = {}\n",
        "        unique_pairs = set()\n",
        "        for line in lines:\n",
        "            tokens = line.split(\" \")\n",
        "            tokens.insert(0, UNK)\n",
        "            tokens.append(EOS)\n",
        "            unique_pairs.update([(tokens[idx], tokens[idx + 1]) for idx in range(len(tokens) - 1)])\n",
        "        total_unqiue_pairs = len(unique_pairs)\n",
        "        for pair in unique_pairs:\n",
        "            counts.update([pair[1]])\n",
        "        for token in vocab:\n",
        "            probs[token] = counts[token] / total_unqiue_pairs\n",
        "        return probs\n",
        "\n",
        "    def string2prefix(self, prefix):\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "        prefix = tuple(prefix)\n",
        "        return prefix\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        if isinstance(prefix, str):\n",
        "            prefix = self.string2prefix(prefix)\n",
        "        return {token: self.get_next_token_prob(token) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        if isinstance(prefix, str):\n",
        "            prefix = self.string2prefix(prefix)\n",
        "        if next_token in self.vocab:\n",
        "            base_term = self.probs.get(prefix, Counter()).get(next_token, 0)\n",
        "            # If we don't have such prefix -> return follow token probability\n",
        "            if not self.probs.get(prefix, False):\n",
        "                return self.models_m.get_next_token_prob(prefix[1:], next_token) if self.n > 1 else base_term\n",
        "\n",
        "            if self.n > 1:\n",
        "                recursion_part = self.models_m.get_next_token_prob(\" \".join(prefix[1:]), next_token)\n",
        "                return base_term + self.delta * self.len_counts.get(prefix, 0) * recursion_part / self.total_counts.get(prefix)\n",
        "            else:\n",
        "                return base_term\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "Jk9EVw1jxrL7"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4-iIdk5-wXk",
        "outputId": "be396e40-088f-4e67-9556-9df5974683b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 2-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 946/946 [00:00<00:00, 156422.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 2-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 946/946 [00:00<00:00, 189756.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 3-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2086/2086 [00:00<00:00, 154956.66it/s]\n"
          ]
        }
      ],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
        "    test_sum = sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab])\n",
        "    assert np.allclose(test_sum, 1), f\"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzkpEJaZ-wXo",
        "outputId": "78f7b659-784b-49d3-beb3-04e8f47cc5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:10<00:00, 950.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N = 1, Perplexity = 2885.69413\n",
            "--------------------------------------------------\n",
            "Train 2-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54176/54176 [00:01<00:00, 49228.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:24<00:00, 410.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N = 2, Perplexity = 385.99528\n",
            "--------------------------------------------------\n",
            "Train 2-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54176/54176 [00:01<00:00, 33866.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 3-gram model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1005464/1005464 [00:06<00:00, 145526.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10250/10250 [00:41<00:00, 246.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N = 3, Perplexity = 294.52713\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = KneserNeyLanguageModel(train_lines, n=n, delta=.7)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"\\nN = %i, Perplexity = %.5f\" % (n, ppx))\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MHnJx03j-on"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
    "notebookPath": "seminar.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}